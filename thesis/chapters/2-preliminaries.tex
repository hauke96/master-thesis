% !TEX root = ../thesis.tex
% !TeX spellcheck = en_US

\section{Geospatial data}

	According to the ISO standard 19109:2015\cite{geolexica-202}, the term \term{geospatial data} refers to \enquote{data with implicit or explicit reference to a location relative to the Earth}.
	In other words, data with addresses, coordinates or other location references belongs to the class of geospatial data, or geodata for short.
	
	Typical examples are customer databases, in which each customer has an address, or meteorological data with coordinates, altitude and timestamp.
	Latter therefore belongs to \term{spatiotemporal data}, because is has an additional time dimension.
	Since addresses are not globally standardized, describe only a vague location and usually belong to a hole area, they are of less importance for this thesis.
	Whenever this thesis mentions geodata, data structures as described below in \cref{subsec:data-structures} are meant.
	
	Next to the mentioned explicit references to locations (such as coordinates), implicit references can be encoded within metadata of datasets as well.
	This is for example the case for the GeoTIFF format, storing location data in the matadata of a normal TIFF image file\cite{ogc-geotiff}.
	The exact coordinate of a data point (in this case a pixel in the TIFF image), must then be calculated using this matadata.

	\subsection{Coordinate reference systems and projections}
	
		Usually coordinates are used to specify exact locations in a specific domain (usually on the surface of the Earth).
		Unfortunately the Earth is not a perfect sphere but rather an ellipsoid.
		But even this ellipsoid is not smooth and has slightly different radii in different regions.
		Furthermore, parts of the surface move relative to each other (continental drift) and therefore change the coordinates of whole regions.
		Even though, the continental plates move slowly, this effect adds up to a significant distance rendering accurate measurements potentially useless\cite[7]{ordenance-survey-booklet}.
		These properties of the surface of Earth make it necessary to have a variety of different reference systems and projections for spatial data.
		
		A \todo{check this whole CRS thing for correctness}\term{coordinate reference system} (CRS) defines the combination of a coordinate system and an approximation of the surface of the Earth.
		The \term{coordinate system} defines, like a mathematical coordinate system, the extend and unit of coordinates.
		Types of coordinate systems are for example cartesian (with meters or foot as unit) and ellipsoidal (with latitude and longitude in degrees)\cite[11-13]{ordenance-survey-booklet}.
		The \term{horizontal datum} (also \term{Terrestrial Reference System} or \term{geodetic datum}) defines the origin and orientation of the coordinate system and is therefore a mapping from coordinate to location on Earth based on a specific ellipsoid.
		Depending on this ellipsoid, the datum either defines a global or local reference system.
		Latter is more accurate in certain places while a global system tries to find a good approximation for every location on Earth.
		For two dimensional Cartesian coordinate systems, a \term*[map projection]{projection} is needed to turn each  spherical location to a two dimensional planar coordinate\cite[17]{ordenance-survey-booklet}.
		
		For visualization purposes, a \term{map projection} turns coordinates of a specific CRS into flat and two dimensional coordinate for a map.
		This may be a different projection as used in a two dimensional Cartesian CRS and therefore transformations between different projections are necessary.
		The map projection used for the final map is independent from the data and only affects the visualization.

	\subsection{Data structures}
	\label{subsec:data-structures}
	
		Spatial data can have a variety of data structures.
		Some of these are used in many or all frameworks and formats, but some are vendor specific.
		This section gives an overview of common data structures in spatial data.
		The \term{simple feature access} (\term*{SFA}) standard by the OGC defines numerous geometry types\cite{ogc-sfa}, but not all of them are implemented in all format and most of them are not relevant for this thesis as well.
		Because the GeoJSON format (described in more detail in \cref{subsubsec:geojson}) implements the most important geometries of the SFA standard\cite[2]{ietf-geojson}, this section gives an overview of the geometry types implemented by GeoJSON.
		
		The first concepts to consider are geometries and features.
		
		A \term{geometry} just represents geometric coordinates and their relation to each other.
		The simplest geometry is a \texttt{Point} with one coordinate.
		Ordering multiple points turns them into a \texttt{LineString} with a certain direction.
		A closed \texttt{LineString} is called \texttt{Polygon} and describes an area.
		Multiple geometries of the same kind can be collected forming a \texttt{MultiPoint}, \texttt{MultiLineString} or \texttt{MultiPolygon}.
		Polygons have an additional property:
		Putting polygons inside polygons forms \enquote{holes} and \enquote{islands}.
		
		According to the OGC, a \term{feature} is an \enquote{abstraction of real world phenomena}\cite[9]{ogc-sfa}, for example a tree, road or lake.
		It therefore consists of a geometry, describing \textit{where} the feature is, with certain attributes (sometimes also called properties), describing \textit{what} the feature represents.
		Such attributes are -- in the case of GeoJSON -- not further defined and can contain arbitrary values.
		Some file formats, however, specify technical restrictions on attributes (for example length restriction in Esri shapefiles as described in \cref{subsubsec:shapefile}) while others try to standardize the allowed keys and values.
		
		A community driven standardization strategy is used by the OpenStreetMap project, where the OSM-Wiki defines certain keys and value but still allows arbitrary tags to exist\cite{osm-wiki-proposal-process}.
		This ensures adaptations to arbitrary real world objects, while having a standard for common and globally used attributes.
		More details on OpenStreetMap are given in \cref{subsec:osm}.
		
		Other systems define a fixed set of allowed and keys and values for attributes.
		For example, the INSPIRE standard of the EU defines \texttt{natureConservation} as one allowed value for the \texttt{SiteProtectionClassification} attribute to mark nature protection areas \enquote{for the maintenance of biological diversity}\cite[31]{inspire-protected-sites}.
			
	\subsection{File formats}
	\label{subsec:file-formats}
	
		Spatial data can be stored in a variety of different file formats with different properties and use cases.
		Next to proprietary formats, there are many open standards.
		The \term[ogc]{Open Geospatial Consortium} (OGC) published open standard definitions and schemas for some of these open file formats.		
		This section gives an overview of some popular formats:
		
		\subsubsection{Esri shapefile}
		\label{subsubsec:shapefile}
		
			The \term[shapefile]{Esri Shapefile} format was developed in the 1990s and is a popular file format and used for example in Esri products like the ArcGIS platform.
			Each shapefile actually consists of multiple files on disk\cite{esri-shapefile-file-ext-spec}.
			Three of them are mandatory containing the geometries (\texttt{.shp}), an index (\texttt{.idx}) and a dBASE table with attributes (\texttt{.dbf}).
			Additional files are optional and can contain additional indices or details about the CRS.
			Even though shapefiles are very popular\cite[356]{spatial-file-formats-trends}, there are several limitations regarding the containing geometries and attributes.
			Only one type of geometry is supported per shapefile, which means a shapefile cannot contain points and polygons at the same time\cite[4]{esri-shapefile-spec}.
			Attributes also have limitations in their size and structure, for example are the attribute keys limited to 10 and values to 254 characters\cite{esri-shapefile-limitations}.
			The default CRS is based on latitudes and longitudes using the NAD 27 datum\cite{esri-shapefile-coordinate-system}.
			
		\subsubsection{KML}
		
			Another popular spatial format is the \term[kml]{Keyhole Markup Language} (KML), which was standardized by the OGC in 2008\cite{ogc-kml-2.2}.
			A KML file consist of a single XML structured file.
			Since it follows the XML standard, which is plain text, geometry and attribute count and lengths are irrelevant.
			KML files may contain more information than just features:
			Features can be grouped and hirarchically organized, temporal data can be added to define a lifetime of a feature and camera views for visualization applications can be defined\cite{ogc-kml-2.3}.
			The CRS can be specified but WGS 84 with latitudes and longitudes is used by default.
		
		\subsubsection{GeoJSON}
		\label{subsubsec:geojson}
		
			Another open but not OGC standardized format is \term{GeoJSON}.
			As the name suggests, a GeoJSON file is valid JSON following the structure defined in \href{https://datatracker.ietf.org/doc/html/rfc7946}{RFC7946}\cite{ietf-geojson}.
			The file may contain a collection of geometries or features but can also just contain a single geometry or feature.
			As in KML files, the length, character choice and amount of attributes or geometries are not restricted.
			The CRS has been set to WGS 84.
			
		\subsubsection{GeoTIFF}
		
			All formats mentioned so far are store vector data, however, raster data can be georeferenced as well.
			One format to store such raster data in form of an image, is the OGC standardized \term{GeoTIFF} format.
			As the name suggests, it is an TIFF formatted image with spatial metadata such as the CRS and a mapping of pixel to coordinates\cite{ogc-geotiff}.
			Typical use cases for such images are aerial or satellite imagery and digital elevation models (DEM).
		
		\subsubsection{Other formats}
		
			There are many more formats for numerous use cases ranging from simple unstandardized to complex, feature rich and fully standardized formats.
			
			A \emph{comma-separated value} (\term*{CSV}) file can also serve as a spatial file format where certain columns contain geographical information.
			The simplicity of this format is its biggest advantage and it is used as transportation format for many public datasets.
			One example is the public available timetable data of the HVV (public transport network in Hamburg, Germany) \cite{hvv-fahrplandaten}.
			This uses the GTFS format, which is based on CSV files and also contains some shapes to describe the route a vehicle takes \cite{google-gtfs}.
			However, there are no popular standard using CSV files for pure spatial data.
			
			The \term{Well-known Text} (\term*{WKT}) format is a simple, text based format specified by the OGC\cite[51]{ogc-sfa}.
			One popular use case is communication with databases.
			One example is PostGIS, an extension to the Postgres database management system, which uses WKT as one exchange format to read and write spatial data\cite{postgis-doc-wkt}.
			The WKT format also exists as a binary version called \term{Well-known Binary} (\term*{WKB}), which consumes less space.
			
			More complex formats are \term{SpatiaLite} and \term{GeoPackage}.
			SpatiaLite specifies an extension to file based SQLite database engine and offers additional geodata specific functionality\cite{spatialite-website}.
			The GeoPackage is based on an SQLite database as well, but it diverged from the SpatiaLite format, for example by using a different WKB encoding\cite{geopackage-faq}.
			
	
	
	\subsection{OGC web services}
	
		Next to the above mentioned local and file based formats, there are web based standards as well.
		The OGC standardized some APIs for web based services.
		Popular standards are the \term{Web Map Service} (\term*{WMS}), \term{Web Map Tile Service} (\term*{WMTS}) and \term{Web Feature Service} (\term*{WFS}).
		
		Each API has a \texttt{REQUEST} parameter used to specify a operation to perform.
		This can be an operation to get metadata of the service itself (for example with \texttt{GetCapabilities}), but can also be a specific operation to interact with the data.
		Each operation accepts additional parameters, for example the extent from with data is requested or the output format.
		
		
		\subsubsection{WMS -- Web Map Service}
		
			A \term*{Web Map Service} provides access to rendered images from geodata but not the the underlying raw data used to create the image.
			However, one optional operation a WMS API might support, is the \texttt{GetFeatureInfo} operation\cite[38]{ogc-wms}, which returns more information about features in the image, but does not necessarily return the raw data.
		
		\subsubsection{WMTS -- Web Map Tile Service}
		
			Similar to a WMS service, the \term*{Web Map Tile Service} specification was created to provide a more scalable service providing rendered maps\cite[12]{ogc-wmts}.
			Each request returns a tile with a fixed bounding box, making server and client side caching possible.
			While a WMS can be used for dynamic and changing data, WMTS focuses on more static data, which can be prerendered to increase performance\cite[13]{ogc-wmts}.
		
		\subsubsection{WFS -- Web Feature Service}
		
			To access raw data, a \term*{Web Feature Service} can be used\cite{ogc-wfs}.
			As the name suggests, it returns features instead of a rendered map.
			Next to query operations like \texttt{GetFeature}, the API specification also describes an optional \texttt{Transaction} operation to change the data.
	
	\subsection{GIS}
	
		A file format alone, as described in the \hyperref[subsec:file-formats]{previous section}, is only useful when it comes to storing or transferring data or ensuring interoperability.
		Processing the data is part of applications referred to as \term[geographic information system]{geographic information systems} (\term*{GIS}).
		
		The term GIS is very broad and describes nearly all systems working with spatial data.
		This includes databases like Postgres with the PostGIS extension, processing tools like GDAL, libraries like the Net Topology Suite, servers like GeoServer and desktop applications like ArcMap or QGIS.
		Latter often combine multiple functionalities for visualization, processing and analysis purposes.
	
	\subsection{OpenStreetMap}
	\label{subsec:osm}
	
		\term{OpenStreetMap} (\term*{OSM}) is a geospatial database that is maintained by a global community and licensed under the \term{Open Data Commons Open Database License} (\term*{ODbL})\cite{osm-wiki-about}.
		It can therefore be used, changed and redistributed as long as a proper attribution is given and results stay under the ODbL\cite{odbl-summary}.
		In 2006, two years after the OSM project started, the OpenStreetMap Foundation was established to perform fund-raising, maintain the servers and also act as a legal entity.
		People contributing to OSM are called \textit{mappers} or simply \textit{contributors}.
		Most of them are volunteers, often mapping their local vicinity or concentrating on specific topics, but a significant amount of changes are contributed by companies with payed mappers\cite{osm-corporate-mappers}.
		
		\subsubsection{Data model}
		
			The model of OSM is much simpler compared to many of the \hyperref[subsec:file-formats]{above mentioned} data formats.
			There are three main data types in OSM: \term{node}, \term{way} and \term{relations}\cite{osm-wiki-data-model}.
			
			Nodes and ways are analogous to \texttt{Point} and \texttt{LineString} from the OGC Simple Feature Access (SFA) specification described in \cref{subsec:data-structures}.
			Areas also exist but they are modeled as closed ways, therefore they do not form a new type of geometry.
			A way is closed, when the first and last coordinates are identical, and thus are analogous to a \texttt{Polygon} in the SFA specification.
			
			Relations form multi-geometries, so they are analogous to \texttt{MultiPoint}, \texttt{MultiLineString} and \texttt{MultiPolygon} of the SFA specification.
			Typical use cases are multipolygons (for example a lake in a forest), turn restrictions and bus routes.
			Each element in a relation can have the optional attribute \texttt{role} specifying \enquote{the function of a member in the relation}\cite{osm-wiki-relation}.
			Example values for this role are \texttt{outer} and \texttt{inner} for rings in a multipolygon or \texttt{stop} and \texttt{platform} in a bus route relation.
			
		\subsubsection{Attributes}
		\label{subsubsec:osm-attributes}
			
			Attributes are called \term[tag]{tags} and are simple unrestricted key-value pairs.
			The Wiki of OpenStreetMap defines many tags with designated values via a special proposal process, but new reasonable values can always be added\cite{osm-wiki-proposal-process}.
			
			Most tags describe the properties of the feature.
			For example a node can have tags making it a restaurant with additional tags for the name, address, the food to get there, the opening hours and if the toilets are barrier free or not.
			
			Some tags, however, like \texttt{area=[yes|no]} or the \texttt{type} key for relations, influence the type of geometry.
			A closed way with \texttt{area=no} does in fast \textit{not} form an area or polygon, but is just a closed line.
			This of course affects visualizations but may also need to be considered in other processing and analysis tasks.
			
			Other tags add metadata to objects, for example the last time the feature was checked, the source or internal notes to other mappers.
			
			There are two ways a combination of key and value becomes an \enquote{official} tag:
			By using it so often that it is de-facto accepted or by a proposal and vote.
			The proposal process tries to organize and professionalize the development of new tasks.
			Not only is a new tagging scheme accepted or rejected by a democratic vote, a mandatory discussion needs to take place earlier.
			Due to the vast amount of contributors, different opinions, different editors and different knowledge about the tagging schemes, sometimes two or more competing schemes evolve (for example \texttt{phone=*} and \texttt{contact:phone=*}).
			Tagging schemes change over time and some may become deprecated, leading to outdated tags on objects.
			
		\subsubsection{Contributions to OSM}
		
			Uploads to OSM always happen in so called \term[changeset]{changesets} combining multiple changed on the map\cite{osm-wiki-changeset}.
			Each changeset can have tags, just like features can.
			However, tags on a changeset specify certain metadata to the change.
			The \texttt{created\_by}, \texttt{comment} and \texttt{source} tags are the most common ones but more details can be added and depends on the editor.
			
			Because some mappers also perform manual quality assurance, each changeset should fulfill certain quality criteria.
			The simplest one is a good changeset comment answering questions like \textit{what changed?} and \textit{why was the change necessary?}.
			For legal reasons, specifying the source is very important since the ODbL is not necessarily compatible with licenses of other data sources.
			Ideally a changeset should be coherent, which means that it should focus on one thematic aspect in one local area.
			Too large and crowded changesets are difficult to maintain.
			
		\subsubsection{Data contained in OSM}
		
			Since OSM has no focus on specific topics and due to the flexible tagging scheme (see \cref{subsubsec:osm-attributes}) nearly anything can be added to OSM.
			However, there are some types of features that are very common according to the \href{https://taginfo.openstreetmap.org/keys}{taginfo.openstreetmap.org}, a service providing basic statistics about the currently used keys and values\cite{taginfo-keys}.
			The statistics cover the whole OSM database and are updated once per day.
			
			According to these statistics, the most common objects are buildings with over 542 million occurrences.
			In fact 6\% of all objects and nearly 60\% of all ways in OSM are buildings.
			Highways (mainly roads and streets but also paths, bridleways, railways and more) are the second common type of features with over 233 million occurrences.
			Addresses are also very common, about 32\% of all nodes and 7\% of all ways have a house number.
			Other area features like forests or lakes exist a lot as well as line features like barriers and waterways.
			
		\subsubsection{Data \textit{not} contained in OSM}
		
			Even though, the tagging scheme can be extended arbitrarily, some data will probably not be added to OSM.
			This can have multiple reasons.
			
			First, some data is too detailed and therefore people will not invest the time necessary to create that data.
			Second, OSM follows some strategies on what to add and what not.
			One important strategy is the verifiability of objects on the ground.
			Anyone visiting a certain place should be able to find the data from OSM and therefore verify its existence and correctness.
			Third, temporary data should not be added since OSM is not a real-time database.
			However, there's not a strict definition when a feature is temporary and when it is (potentially) permanent.
			And fourth, data under a uncompatible license will not be added.
			This also includes data from many public authorities and nearly all companies.
			
			Unfortunately, data relevant to routing is often added to roads and paths only but not to areas.
			This includes primarily accessibility and surface information.
			However, latter can potentially be inferred or approximated from other tags on a polygon, for example \texttt{surface=grass} can be interred from \texttt{natural=grassland} and \texttt{access=yes} from \texttt{place=square}.
			
			Private areas, however, do often not exist at all or have very little details.
			This is mainly due to reason two from above and rooted in a common sense of privacy towards companies and residents.
			But also the areas in question are often not accessible to everyone, rendering them unverifiable for unauthorized people.
			
		\subsubsection{Data used in this thesis}
		
			This thesis uses OSM data for routing purposes.
			Since geometric routing is the major topic, certain features in OSM are used as obstacles to navigate between them.
			
			\todo[inline]{Describe the exact features used during evaluation.}

\section{Routing}

	% fastest, shortest path, all shortest...
	
	\subsection{Theoretic considerations}
	\label{subsec:routing-theoretic-considerations}
	
		Formally, routing describes the process of finding an optimal path within a graph $G=(V, E)$ from a start location $s$ to a target location $t$ with $s, t \in V$.
		A path is an ordered list of connected vertices $p=\left\langle v_0, v_1, \dots v_n \right\rangle$, so for an edge $(v_i, v_j)$ of consecutive vertices, $(v_i, v_j) \in E$ must hold.
		In geometric routing approaches (see \cref{subsec:geometric-routing}), the result may be a list of coordinates that do not need to be within $V$.
		
		Often, \enquote{optimal} means shortest, whereby the length of an edge is considered as arbitrary weight and is therefore not bound to geographic lengths of edges.
		In theoretic computer science, the fundamental problem behind routing algorithms is the \term{shortest path problem}.
		However, the shortest path problem exists in several variants\cite[644]{cormen-introduction-to-alg}:
		
		\subsubsection{\term*[single-source shortest paths]{Single-source shortest paths}}
		\label{subsubsec:single-source-shortest-path}
		
			One source vertex is given and the shortest paths to all other vertices should be determined.
			Algorithms that solve this problem are for example Dijkstra, which will be described in detail in \cref{subsubsec:dijkstra}, or the Bellman-Ford algorithm, which can handle negative edge weights \cite[651]{cormen-introduction-to-alg}.
		
		\subsubsection{\term*[single-destination shortest paths]{Single-destination shortest paths}}
		
			This is the opposite to the above problem.
			All shortest paths to a specific vertex from any other vertex should be found.
			However, no new algorithms are needed to solve this problem.
			Instead, the direction of each edge can be reversed turning this problem into the single-source problem.
		
		\subsubsection{\term*[single-pair shortest paths]{Single-pair shortest paths}}
		
			Like the single-source but shortest paths from one source to one target vertex are requested.
			Even though algorithms like TRANSIT (described in \cref{subsubsec:transit}) solve this problem efficiently, other approaches like Dijkstra, which actually solves the single-source problem, are used more often.
		
		\subsubsection{\term*[all-pair shortest paths]{All-pair shortest paths}}
		\label{subsubsec:all-pair-shortest-path}
		
			This problem is similar to the single-pair problem but is not restricted to a single source vertex.
			A naive approach would be to use an algorithm solving the single-pair problem on each vertex, but there are faster ways to solve this.
			Two popular algorithms solving this problem are the Floyd-Warshall algorithm, allowing negative edge weights by assuming no negative cycles exist \cite[693]{cormen-introduction-to-alg}, and Johnson's algorithm, which also detects negative cycles and terminates in such case \cite[700]{cormen-introduction-to-alg}.

	\subsection{Graph based routing}
		
		As describes above in \cref{subsec:routing-theoretic-considerations}, graph based routing uses a Graph $G=(V, E)$ with vertices $V$ and edges $E$ and finds a path within that graph.
		The optimal route is determined based on weights $w : E \rightarrow \mathbb{R}$ on each edge, so an edge $(u, v) \in E$ has weight $w(u, v)$.
		Usually, a path with the minimal weight should be determines, meaning for a path $p$ of vertices, $w(p) = \sum{w(v_{i-1}, v_i)}$ should be minimal\cite[645]{cormen-introduction-to-alg}.
		
		The function $w$ needs to be chosen based on the situation, vehicle and other desirable properties.
		These criteria are bundled in a \term{routing profile}, which maps attributed from the input data to a certain weight.
		Such weight might be the predicted speed\cite{graphhopper-profile-bike-speeds}, the length of the edge\cite{graphhopper-profile-shortest} or just represent a certain number based on a combination of attributes\cite{graphhopper-profile-short-fastest}.
		
		Routing engines, such as Graphhopper, come with predefined profiles for different modalities and situations.
		For example the \texttt{car\_delivery} profile is made for delivery services using cars and therefore allowing to drive on private roads\cite{graphhopper-routing-profiles}.
		
		To enhance performance, speed up methods were developed to be able to perform fast routing queries on datasets of continental sizes.
		Popular methods are \term{contraction hierarchies}, the usage of \term{landmarks}, \term[transit node]{transit nodes} and \term[hub label]{hub labels}.
		Even though there are several more approaches to increase performance, the mentioned techniques will be described in the following sections.
		
		\subsubsection{Dijkstra}
		\label{subsubsec:dijkstra}
		
			Dijkstra's algorithm, often just called \term{Dijkstra}, is an rather old algorithm from the 1950s solving the single-source shortest paths problem.
			It actually creates a spanning tree such that all paths in the tree are shortest paths from a start vertex $s$.
			Despite its age, Dijkstra's algorithm and optimized versions of it are frequently used in science and real life applications and will be mentioned quite often in this thesis.
			\Cref{alg:dijkstra} is an already enhanced version using a priority queue for the vertices\cite[658]{cormen-introduction-to-alg}.
			
			\begin{algorithm}[h]
				\begin{algorithmic}[1]
					\ForAll{vertices $v \in V$}
						\State Set shortest path distance $v.d = \infty$ and predecessor $v.\pi = undefined$
					\EndFor
					\State Insert all vertices into distance based min-queue $Q$
					\State
					\While{$Q \neq \emptyset$}
						\State Get the next closest vertex: $u = Q.min$ \label{alg:dijkstra:extract-queue}
						\ForAll{adjacent vertices $v$ of $u$}
							\If{the path to $v$ via $u$ is shorter, so if $v.d > u.d + d(u, v)$}
								\State Set $v.d = u.d + d(u, v)$ and $v.\pi = u$
							\EndIf
						\EndFor
					\EndWhile
				\end{algorithmic}
				\caption{Pseudocode of an slightly optimized version of Dijkstra's algorithm.}
				\label{alg:dijkstra}
			\end{algorithm}
		
			Once a vertex is taken from the queue, it is considered as \emph{visited}.
			It can be proven that for each visited vertex $v$, the distance $v.d$ is optimal \cite[659-661]{cormen-introduction-to-alg}.
			This also means following back the predecessor relation $v.\pi$ yields the shortest path.
			Therefore, placing an exit condition after line \ref{alg:dijkstra:extract-queue} can increase performance when a target vertex is known.
		
		\subsubsection{A*}
		\label{subsubsec:astar}
		
			The A* shortest path algorithm was introduced in 1968 and uses a heuristic $h : V \rightarrow \mathbb{R}$ to decide that vertex to visit next.
			
			It is rather simple and \cref{alg:astar} expresses it in pseudocode.
			The path from $s$ to $t$ is requested and $h$ is the heuristic.
			
			\begin{algorithm}[h]
				\begin{algorithmic}[1]
					\State Mark $s$ as \emph{open}
					\State Set $u = s$
					\While{$u \neq t$}
						\State Mark $u$ as \emph{closed}
						\State Store $u$ to output graph $G_s$
						\State Mark all non-closed successors of $u$ as \emph{open}
						\State Mark each closed successor $u'$ of $u$ as \emph{open} when $h(u')$ became lower since it has been marked as \emph{closed}
						\State Select open vertex $u$ with minimal $h(u)$
					\EndWhile
					\State Take $G_s$, follow $t$ back to the source $s$ and output the path
				\end{algorithmic}
				\caption{Pseudocode of the originally proposed A* algorithm.}
				\label{alg:astar}
			\end{algorithm}
		
		\subsubsection{Contraction Hierarchies}
		\label{subsubsec:ch}
		
			Contraction hierarchies are based on the idea to reduce the amount of hops made by a routing algorithm by adding so called \emph{shortcut edges} between nodes \cite{geisberger-contraction-hierarchies}.
			Consider a vertex $u$ with incoming edge $(v, u)$ and outgoing edge $(u, w)$.
			Vertex $u$ is \emph{contracted} by adding a new edge $e = (v, w)$ with weight $w(e) = w(v, u) + w(u, w)$.
			If $e$ already exists with a higher weight, its weight is decreased accordingly.
			
			A very important part of this technique is the selection of nodes to contract \cite[14]{geisberger-contraction-hierarchies}.
			The nodes are selected by a total order $<$ defining a level of importance to each node.
			Selecting the correct order is difficult and heuristics are used to approximate a perfect ordering.
			Such heuristic can for example be the difference in edge counts from before and after contraction, the uniformity of the distribution of contractions in the Graph or the resulting size of the query search space.
			Multiple heuristics can be combined leading to even better results \cite[49]{geisberger-contraction-hierarchies}.
			The vertices are stored in a priority queue based on a linear combination of all used heuristics and the most attractive vertex is processed first.
			
			The contraction hierarchy can be used with an adjusted bidirectional Dijkstra algorithm \cite[29-30]{geisberger-contraction-hierarchies}.
			A query for the shortest path is done on two graphs, an upward and downward graph.
			The upward graph contains only edges from lower to higher order nodes and the downward graph only from higher to lower order nodes.
			The query finishes, when the two queries (bidirectional) meet \emph{and} there is no lower weighted edge to nodes in the priority queue.
			All shortcut edges must now be recursively relaxed to create the actual shortest path.
		
		\subsubsection{Landmarks}
		
			A popular usage of so called \term[landmark]{landmarks} is the ALT algorithm, which stands for A*, landmarks and triangle inequality \cite{goldberg-landmarks}.
			As described in \cref{subsubsec:astar}, the A* algorithm needs a heuristic leading the search for the shortest path towards the target vertex.
			This heuristic is often the euclidean distance divided by a certain speed.
			This is simple but rather inaccurate.
			
			A better, but also more complex, approach is the use of landmarks.
			Landmarks are vertices to which all distances are calculated in a precomputation step.
			
			Consider a distance function $d : V \times V \rightarrow \mathbb{R}$, two vertices $u$ and $v$ as well as a landmark vertex $l$.
			The triangle inequality says $d(u,v) \geq d(u,l) - d(v,l)$ for the distances \emph{to} $l$ but also $d(u,v) \geq d(l,u) - d(l,v)$ for distances \emph{from} $l$.
			This can be used to build the heuristic $h(u) = \max{d(u,l)-d(v,l), d(l,u)-d(l,v)}$ for all landmarks $l$ \cite[6-7]{goldberg-landmarks}.
			
			Thanks to the triangle inequality, the A* algorithm can use this to determine good choices for next vertices.
			Since the heuristic consist of the tightest bound by using the $\max$ function, the value estimated distance to $t$ using the heuristic is likely to increase or decrease when the actual distance does the same.
		
		\subsubsection{Transit nodes}
		\label{subsubsec:transit}
		
			The routing algorithm called \term{TRANSIT} uses specially selected vertices, so called \term[transit node]{transit nodes}, and a the distances between them and all other vertices to find shortest paths very quickly \cite{bast-transit}.
		
			First, the graph is divided into smaller cells.
			Local queries using Dijkstra are then used to determine the transit nodes.
			A good transit node is a vertex through which many shortest paths lead to vertices in more distant cells \cite[6]{bast-transit}. 
			Since Dijkstra solved the single-source shortest paths problem (described in \cref{subsubsec:single-source-shortest-path}), the distances from any normal vertex to its nearest transit nodes are stored.
			The final step of the preprocessing is the calculation of the distances between all transit nodes, which means solving the all-pair shortest path problem as described in \cref{subsubsec:all-pair-shortest-path}.
			
			Having the distances between transit nodes and the distances from any other vertex to its closest transit nodes makes it easy to compute distances between arbitrary vertices.
			This is done by the two following routines.
			
			The first routine calculates just the distance of the shortest path using the precomputed values from above \cite[7-8]{bast-transit}.
			Given a start vertex $src \in V$ and target vertex $trg \in V$, the distance $d(src, trg)$ needs to be known.
			All distances $d(src, t_{src})$ for each transit node $t_{src} \in T_{src}$, which are the closest transit nodes of $src$, are precomputed an known.
			Same holds for all distances $d(t_{trg}, trg)$ from each closest transit node $t_{trg} \in T_{trg}$ to $trg$.
			The minimal distance between $src$ and $trg$ can be found by going through all pairs of the closest transit nodes $t_{src} \in T_{src}$ and $t_{trg} \in T_{trg}$ and calculating $d(src, trg) = d(src, t_{src}) + d(t_{src}, t_{trg}) + d(t_{trg}, trg)$.
			
			The second routing is the actual query routine, which returns the edges of the shortest path, is quite simple \cite[8]{bast-transit}:
			First, calculate $d(src, trg)$ and let $u$ be the currently visited vertex, initially $u = src$.
			The distance from $u$ to the target can be calculated by $d(u, trg) = d(src, trg) - d(src, u)$ of which $d(src, u)$ is known since we started at $src$ and can simply remember the traveled distance so far.
			Next, find the adjacent vertex $v$ of $u$ for which $d(u, trg) = d(u, v) + d(v, trg)$ holds.
			The distance $d(v, trg)$ can easily be computed with the above mentioned routine.
			Finally, store $u$ to a list, let $u$ the the vertex $v$ and repeat the described steps.
			
			After reaching the target, the list of stored vertices build the shortest path.
		
		\subsubsection{Hub labels}
		
			The \term{hub label} strategy is a labeling algorithm calculating labels $L(v)$ for each vertex $v$.
			Each label contains a set of vertices (so called \emph{hubs}) and the length of the shortest path to each of the hubs.
			A label must fulfill two main properties \cite[11]{bast-transportation-networks}.
			
			First, the distance $d(u, v)$ from vertex $u$ to $v$ must be determinable using only their labels $L(u)$ and $L(v)$.
			Since $u$ and $v$ can be arbitrary vertices, the vertex $v$ is not necessarily within $L(u)$.
			To not traverse through all labels to find $v$ from $u$, the second property (so called \emph{cover property}) is very important:
			For two arbitrary vertices $u$ to $v$, the set $L(u) \cap L(v)$ must contain at least one common hub $h$ on the shortest path from $u$ to $v$.
			This can be used to solve fulfill the first property.
			
			The labels can be determined with the mechanism from contraction hierarchies (see \hyperref[subsubsec:ch]{above}) forming the upward graph.
			In fact, any order on vertices can be used to create a correct labeling.

			Routing requests for a shortest path from $u$ to $v$ are simple and fast to answer:
			Get the labels $L(u)$ and $L(v)$ and get the common hubs in them.
			When storing the hubs in a sorted list, building the intersection can be done in linear time time.
			Find the hub pair $h_u$ and $h_v$ in the intersection with the minimum distance sum and return their paths.
			
			Even though the requests can be answered incredibly fast, actually faster than probably any other method, it needs a lot of space and preprocessing time \cite[23]{bast-transportation-networks}.
		
		\subsubsection{Compressed path databases}
		
			A \term{compressed path database} (\term*{CPD}) itself is not directly a routing algorithm but rather a technique to efficiently store shortest paths between points \cite{botea-cpd-2013}.
			In other words, the solution of the all-pair shortest paths problem from \cref{subsubsec:all-pair-shortest-path} can be stored in a CPD.
			
			The simplest form of a CPD works on a grid rather than a graph, where the number of neighbors is limited and visualization and compression is simpler.
			Such grid based CPDs actually consist of $n$ many grids (for $n$ cells), each one storing movement information from a fixed cell $v$ to all other cell.
			Movement information here means, that only a single step is stored.
			
			Let us consider the following example:
			The path from cell $u$ to $v$ is requested, so the grid $u$ is used first.
			On that grid, the cell for $u$ is empty (since it is the root cell) and the cell for $v$ stores the first step to take in order to get from $u$ to $v$.
			This first step (on a grid for example the direction \enquote{north east}) leads to a new cell $u'$.
			The grid for $u'$ is used and again the cell for $v$ tells the algorithm what direction to take.
			This goes on until the cell $v$ is found.
			
			The construction of a CPD is similarly simple \cite[2]{botea-cpd-2013}.
			For each vertex $v$, the shortest paths to all other vertices are calculated, which can be done using Dijkstra or any other single-source or all-pairs shortest path algorithm.
			After converting the shortest paths into a compatible format (in the above case a grid), the data needs to be compressed.
			Otherwise the data becomes too large as the space requirement is in $\bigo{n^2}$ or even $\bigo{n^2 \log n}$ \cite[1]{botea-cpd-2013}.
			
			There are multiple possible ways to compress a raw CPD.
			The original paper uses a decomposition of rectangles with common movement directions to reduce the amount of redundant data \cite[1]{botea-cpd-2011}.
			A new approach presented two years later uses general purpose compression techniques like run-length encoding (RLE) and sliding-window compression (SWC) as well as special list-trimming with default movements.
			
			The original approach already showed good results.
			Using the originally proposed compression (rectangle decomposition only) on a road network yields a CPD with a compression factor of about 180 \cite[4]{botea-cpd-2013}.
			Compressing the same network with all mentioned techniques together can decrease the size even further to a CPD, which is another 5.3 times smaller.
			This is a factor of 950 compared to an uncompressed database.
	
	\subsection{Geometric routing}
	\label{subsec:geometric-routing}
	
		Geometric routing determines shortest paths via open spaces by avoiding obstacles.
		Because this way of routing is based on geometric properties, attributes of edges (for example the surface conditions) are of no use.
		There are to main strategies for finding shortest paths:
		Create edges in open spaces or create a shortest paths map, a structure similar to CPDs.
		
		The first mechanism creates edges and then utilizes a normal graph based routing algorithm to actually find the shortest path.
		There are multiple ways on how to create such edges.
		A well known and often used mechanism creates a \term{visibility graph}, which is a normal graph with edges between vertices that are visible to each other.
		Or in other words, for any two $u, v \in V$ there is an edge $(u, v) \in E$ if and only if there is \textit{no} obstacle intersecting this edge.
		Since a visibility graph might become very large, some edges might be removed or other strategies are more efficient.
		In fact, the worst-case of a visibility graph is a complete graph with $\frac{n^2 - n}{2}$ edges.
		Alternatives to the visibility method are for example based on Voronoi diagrams or other skeletonization methods\cite[219-220]{graser-osm-open-spaces}.
		However, routing results might not be optimal anymore, since a straight edge between visible vertices is the shortest connection one might get and those alternatives might not create such short edges\cite[223]{graser-osm-open-spaces}.
		Another aspect of graph generating methods is, that the start and target locations must be on the graph to be precisely reachable by routing algorithms.
		
		The second strategy creates a map of predecessor vertices, which can be used to determine shortest paths from a source vertex $s$ by following the predecessor relations from any given location.
		This map can also be seen as a tree with $s$ as its root and each region as a leaf.
		Such a tree might remind one of Dijkstras algorithm, which creates such a shortest path tree as well.
		Due to this similarity, this strategy is also called the \term{continuous dijkstra} paradigm\cite[648]{mitchell-discrete-geodesic}.
		The key difficulty of this approach is the propagation of wavefront through space and their interactions with obstacles and other wavefronts\cite[3]{hershberger-suri}.
		A well fitting analogy for this approach is the propagation of sound or water waves traveling through space, folding around obstacles and finally reaching the target.
		Recursively following each wave back to its origin yields the shortest path from the source to the target.

\section{Agent based systems and simulations}

	Simulating complex systems with numerous individuals is a complicated task.
	Agent-based models help to break down this complexity by simulating each individual separately with its own behavior and decision-making. \cite{macal2014introductory}
	Interactions are also an essential part of such simulating and affect other agents as well as the environment.
	
	The modeling of agents can be more or less complex.
	Basic agents can interact with other agents and the environment, are able to make decisions on their own and are autonomous, meaning there is no controlling unit commanding agents to act.
	More complex agents may have a specific goal, are able to adapt themselves to their environment and thus must be modeled with a memory.
	
	An agent might be a simulated person but could also be a company, car or other non-living part of the simulated world.
	Many simulations use pedestrians as agents to better understand human behaviors but many more scenarios are thinkable\cite[8]{macal2014introductory}.
	This often involves a spatial environment where agents can move and interact with each other.
	Since uncontrolled movement is rather uninteresting, most of the time, algorithms finding optimal paths are involved\cite{kneidl-borrmann-hartmann-navigation,gloor-hybrid-pedestrian-routing,teknomo-millonig-routing}.
	
	% TODO Describe MARS here or later?