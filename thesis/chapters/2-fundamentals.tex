% !TEX root = ../thesis.tex
% !TeX spellcheck = en_US

This chapter introduces basic concepts relevant for this thesis.
Primarily, this includes geodata, routing algorithms as well as agent-based simulations.
The section about geodata gives a broad overview of coordinate systems, file formats, standards and OpenStreetMap as one important source of geospatial data.
Such data can be used to find shortest paths between two locations using routing algorithms.
Two main strategies exist to find such paths, graph-based and geometric routing, and both will be covered in this chapter.
agent-based simulations play an important role in pedestrian simulations and often use these routing techniques.
Hence, the topic of agents and simulations will be covered as well.

\section{Geospatial data}

	According to the ISO standard 19109:2015\cite{iso-19109}, the term \term{geospatial data} refers to \enquote{data with implicit or explicit reference to a location relative to the Earth}.
	In other words, data with any kind of address, coordinate or other location reference belongs to the class of \term{geospatial data}, or \term{geodata} for short.
	
	Examples are customer information (address information of each customer), meteorological data (measured temperature and humidity with a coordinate and altitude) and of course road and path data suitable for routing.
	Since addresses are not globally standardized, describe only a vague location and usually belong to a hole area, they are of less importance for this thesis.
	Geodata with an additional time dimension is also categorized as \term{spatiotemporal data}\cite{iso-19108} and even though the time dimension is relevant to many real world use cases, this type of data is of less importance for this thesis as well.
	Whenever this thesis mentions the term \emph{geodata}, data structures as described below in \cref{subsec:data-structures} are meant.
	
	Next to the mentioned explicit references to locations, such as coordinates, implicit location references can be encoded within metadata of datasets as well.
	This is for example the case with the image-based GeoTIFF format, which stores the geographic extent of the image in the metadata of a normal TIFF image file\cite{ogc-geotiff}.
	The exact coordinate of a data point (in this case a pixel in the TIFF image), must then be calculated using this metadata.

	\subsection{Coordinate reference systems and projections}
	
		Coordinates are used to specify exact locations on Earth.
		However, the Earth is not a perfect sphere but rather an ellipsoid.
		But even this ellipsoid is not smooth and has slightly different radii in different regions.
		Furthermore, most regions on Earth move (continental drift) and therefore constantly change their positions and distances relative to each other.
		Even though, the continental plates move slowly, this effect adds up to a significant distance rendering very accurate measurements potentially useless over time \cite[7]{ordenance-survey-booklet}.
		These properties of the surface of Earth make it necessary to have a variety of different reference systems and projections for spatial data covering different parts of the planet.
		
		A \term{coordinate reference system} (CRS) defines the combination of a coordinate system and an approximation of the surface of the Earth.
		The \term{coordinate system} defines the extent and unit of coordinates.
		Types of coordinate systems are for example Cartesian (coordinates in meters or foot) and ellipsoidal (coordinates in degrees) \cite[11-13]{ordenance-survey-booklet}.
		The \term{horizontal datum} (also \term{Terrestrial Reference System} or \term{geodetic datum}) defines the origin and orientation of the coordinate system and is therefore a mapping from a numeric coordinate to a real location on Earth based on a specific ellipsoid.
		
		Since the Earth is a spherical body, the horizontal datum is always tied to an ellipsoid.
		\term[ellipsoid]{Ellipsoids} are deformed spheres, which describe either the whole Earth or a part of it, e.g. a single continent or country, as accurate as possible.
		Depending on this ellipsoid, the horizontal datum either defines a global or local reference system.
		Latter is more accurate in certain places while a global system tries to find a good approximation for every location on Earth.
		Cartesian coordinate systems with two dimensions only work on flat surfaces, therefore a \term*[map projection]{projection} is required to turn each spherical location on the ellipsoid into a two dimensional planar location in the coordinate system \cite[17]{ordenance-survey-booklet}.
		
		Next to the horizontal datum, a \term{vertical datum} can be used to define the height of data \cite{ordenance-survey-booklet}.
		This is done by defining a so called \term{geoid}, which defined the level of zero height.
		Such a geoid can be modeled using another ellipsoidal sphere but that would result in very inaccurate data.
		Instead, local reference levels for height data are used, such as the Ordnance Datum Newlyn (\term*{ODN}) for Britain or Normalhöhennull (\term*{NHN}) for Germany.
		Since it is often based on the water level of oceans, the level of zero height on landmasses is extrapolated based on the gravitational field of the earth.
		
		For visualization purposes, a \term{map projection} might be needed to translate coordinates from the CRS of the data into the coordinate system used of the map.
		The map can be two dimensional but digital maps can also be rendered on a three dimensional sphere.
		Since the projections from the data and map can differ, transformations between the projections are needed.
		
		For better communication, the European Petroleum Survey Group (\term*{EPSG}) created a database with definitions for coordinate reference systems and transformations \cite{epsg}.
		Each entry in this database has an ID with the prefix \enquote{EPSG:} followed by a numeric code, for example EPSG:4326 for the global and widely used WGS 84, a global ellipsoidal horizontal datum used in the GPS.
		
	\subsection{Data structures}
	\label{subsec:data-structures}
	
		Spatial data can be stored in a variety of data structures.
		Some of these are used in many frameworks, libraries and formats, but some are vendor specific.
		This section gives an overview of common data structures in spatial data.
		
		The \term{Simple Feature Access} (\term*{SFA}) standard by the Open Geospatial Consortium (\term*{OGC}) defines numerous geometry types\cite{ogc-sfa}, however, not all of them are implemented in all formats and most of them are not relevant for this thesis.
		Because the widely used GeoJSON format (described in more detail in \cref{subsubsec:geojson}) implements the most important geometries of the SFA standard\cite{ietf-geojson}, this section gives an overview of the geometry types implemented by GeoJSON.
		
		The main concepts considered in this section are geometries and features.
		
		\subsubsection{Geometries}
		
			A \term{geometry} just represents geometric coordinates and their relation to each other.
			Common types of geometries are:
			\begin{description}
				\item[Point] Simple geometry type with only one coordinate.
				\item[LineString] An ordered list of multiple coordinates building a line with a certain direction.
				\item[Polygon] An ordered list of coordinates of which the first and last coordinates are equal and thus form an area. A polygon might consist of additional polygons inside of it forming holes. Polygons inside such holes form islands, which in turn can have holes again.
				\item[Multi-Geometries] Grouping geometries of the same type together forms multi-geometries, such as \texttt{MultiPoint}, \texttt{MultiLineString} and \texttt{MultiPolygon}. They can be used to share properties of the contained geometries.
			\end{description}
		
		\subsubsection{Features}
		
			According to the OGC, a \term{feature} is an \enquote{abstraction of real world phenomena}\cite{ogc-sfa}, for example a tree, road or lake.
			It therefore consists of a geometry, describing \textit{where} the feature is, and certain attributes (sometimes called \enquote{properties} or \enquote{tags}), describing \textit{what} the feature represents.
			In the case of GeoJSON, these attributes consist of any number of key-value pairs that can contain arbitrary entries.
			Some other file formats might have technical restrictions regarding possible attributes (e.g. the number of bytes per value) while others try to standardize the allowed keys and values.
		
		\subsubsection{Standardization}
		
			Standards exist to keep a data source consistent but to also allow interoperability between systems.
			Common standards were defined by organizations like the OGC but companies or government agencies define standards as well.
			
			A widely used proprietary standard established the company Esri Inc. is the Shapefile file format.
			This format will be covered in more detail in \cref{subsubsec:shapefile}.
			
			One example for a governmental standard, is the INSPIRE standard.
			It was established based on an initiative of the European Commission and defines fixed attributes with keys and values as well as specifications for metadata.
			This standard works on a high level of detail with numerous separate specifications for different types of features, for example there exists a single specification just for protected sites \cite[31]{inspire-protected-sites}.
		
			A standardization strategy based on democratic votes is used by \term*{OpenStreetMap} (\term*{OSM}), a crowdsourced spatial database (covered in \cref{subsec:osm} with more details).
			The standardization of OSM takes place in the projects wiki, which defines certain keys and values but also explicitly allows arbitrary tags (key-value pairs) to ensure a high degree of flexibility \cite{osm-wiki-proposal-process}.
			
	% TODO kürzen
	\subsection{File formats}
	\label{subsec:file-formats}
	
		Spatial data can be stored in a variety of different file formats with different properties and use cases.
		Next to proprietary formats, there are many open standards.
		The \term[OGC]{Open Geospatial Consortium} (OGC) published open standard definitions and schemas for some of these open file formats.		
		This section gives an overview of some popular formats:
		
		\subsubsection{Esri shapefile}
		\label{subsubsec:shapefile}
		
			The \term[shapefile]{Esri Shapefile} format (often just called shapefile) was developed in the 1990s by the company Esri and is a popular file format not only used in Esri products like the ArcGIS platform.
			Each shapefile actually consists of multiple files on disk\cite{esri-shapefile-file-ext-spec}.
			Three of them are mandatory containing the geometries (\texttt{.shp}), an index (\texttt{.idx}) and a dBASE table with attributes (\texttt{.dbf}).
			Additional files are optional and can contain additional indices or details about the CRS (\texttt{.prj}).
			Shapefiles are very popular\cite{spatial-file-formats-trends} but have several significant technical limitations regarding the containing geometries and attributes.
			
			Even though the first byte of every geometry in the shapefile encodes its type, the standard allows only one type of geometry per shapefile \cite{esri-shapefile-spec}.
			Storing for example points and polygons would require two separate shapefiles even though the data might belong together.
			Attributes have limitations in their size and structure.
			One major limitation is the maximum length of 10 characters per attribute name and a maximum of 254 characters per value \cite{esri-shapefile-limitations}.
			The maximum number of attributes per geometry is limited by 255.
			The default CRS is based on latitudes and longitudes using the NAD 27 datum\cite{esri-shapefile-coordinate-system}.
			
		\subsubsection{KML}
		
			Another popular spatial format is the \term[kml]{Keyhole Markup Language} (KML), which was standardized by the OGC in 2008\cite{ogc-kml-2.2}.
			A KML file consist of a single XML structured file.
			Since it follows the XML standard, which is plain text, the amount of geometries and attributes are not restricted.
			KML files may contain more information than just features:
			Features can be grouped and hierarchically organized, temporal data can be added to define the lifetime of a feature and camera views for visualization applications can be defined \cite{ogc-kml-2.3}.
			The CRS can be specified but WGS 84 with latitudes and longitudes is used by default.
		
		\subsubsection{GeoJSON}
		\label{subsubsec:geojson}
		
			Another open but not OGC standardized format is \term{GeoJSON}.
			As the name suggests, a GeoJSON file follows the JSON specification as defined in \href{https://datatracker.ietf.org/doc/html/rfc7946}{RFC7946} \cite{ietf-geojson}.
			The file can contain a collection of features (object of type \texttt{FeatureCollection}), where each feature contains a geometry, or the file just contains a list of geometries (\texttt{GeometryCollection}, which itself is a geometry type) without attributes.
			The length of attributes, amount of attributes, character choice and amount of geometries are not restricted.
			The CRS is fixed and has been set to WGS 84.
			
		\subsubsection{GeoTIFF}
		
			All formats mentioned so far store vector data, however, raster data can be georeferenced as well.
			One format to store such raster data in form of an image, is the OGC standardized \term{GeoTIFF} format \cite{ogc-geotiff}.
			As the name suggests, it is an TIFF formatted image with spatial metadata such as the CRS and a mapping of pixel to coordinates.
			Typical use cases for such images are aerial or satellite imagery and \term[digital elevation model]*{digital elevation models} (\term*{DEM}).
		
		\subsubsection{Other formats}
		
			There are many more formats for numerous use cases ranging from simple unstandardized to complex, feature rich and fully standardized formats.
			
			A \emph{comma-separated value} (\term*{CSV}) file can also serve as a spatial file format where certain columns contain geographical information.
			The simplicity of this format is its biggest advantage and it is used as transportation format for many public datasets.
			One example is the public available timetable data of the HVV (public transport network in Hamburg, Germany) \cite{hvv-fahrplandaten}.
			This uses the \term{GTFS} format, which is based on CSV files (even though each file has the ending \texttt{.txt}) and also contains a \texttt{shapes.txt} CSV file to describe which routes the vehicles take \cite{google-gtfs}.
			However, there is no popular standard using CSV files to store spatial data only.
			
			The \term{Well-known Text} (\term*{WKT}) format is a simple, text-based format specified by the OGC\cite[51]{ogc-sfa}.
			One popular use case is communication with databases.
			One example is PostGIS, an extension to the PostgreSQL database management system, which uses WKT as one exchange format to read and write spatial data\cite{postgis-doc-wkt}.
			The WKT format also exists as a binary version called \term{Well-known Binary} (\term*{WKB}), which consumes less space.
			
			More complex formats are \term{SpatiaLite} and \term{GeoPackage}.
			SpatiaLite specifies an extension to file-based SQLite database engine and offers additional geodata specific functionality\cite{spatialite-website}.
			The GeoPackage format is very similar to SpataLite as it is based on an SQLite database as well, but it diverged from the SpatiaLite format, for example by using a different WKB encoding\cite{geopackage-faq}.
			
	\subsection{OGC web services}
	
		Next to the above mentioned file-based formats, there are web-based standards as well.
		The OGC standardized some APIs for web-based services.
		Popular standards are the \term{Web Map Service} (\term*{WMS}), \term{Web Map Tile Service} (\term*{WMTS}) and \term{Web Feature Service} (\term*{WFS}).
		
		Each API has a \texttt{REQUEST} parameter used to specify a operation to perform, for example get metadata with \texttt{GetCapabilities} or features with \texttt{GetFeature}.
		Each operation accepts additional parameters, for example the extent from with data is requested or the output format.
		
		\subsubsection{WMS -- Web Map Service}
		
			A \term*{Web Map Service} provides access to rendered images but not the the underlying raw data \cite{ogc-wms}.
			However, one optional operation a WMS API might support, is the \texttt{GetFeatureInfo} operation, which returns more information about features in the image, but does not necessarily return the raw data.
		
		\subsubsection{WMTS -- Web Map Tile Service}
		
			Similar to a WMS service, the \term*{Web Map Tile Service} specification was created to provide a more scalable service providing rendered map tiles \cite{ogc-wmts}.
			Each request returns a tile with a fixed bounding box, making caching on the server and client side possible.
			While a WMS can be used for dynamic and changing data, WMTS focuses on more static data, which can be prerendered to increase request performance.
		
		\subsubsection{WFS -- Web Feature Service}
		
			To access raw data, a \term*{Web Feature Service} can be used \cite{ogc-wfs}.
			As the name suggests, it returns features instead of a rendered map.
			Next to query operations like \texttt{GetFeature}, the API specification also describes an optional \texttt{Transaction} operation to change the data.
	
	\subsection{GIS}
	
		A file format alone, as described in the \hyperref[subsec:file-formats]{previous section}, is only useful when it comes to storing or transferring data ensuring interoperability.
		Processing the data is part of applications referred to as \term[geographic information system]{geographic information systems} (\term*{GIS}).
		
		The term GIS is very broad and describes nearly all systems working with spatial data.
		This includes databases like Postgres with the PostGIS extension, processing tools like GDAL, libraries like the Net Topology Suite, servers like GeoServer and desktop applications like ArcMap or QGIS.
		Latter often combine multiple functionalities for visualization, processing and analysis purposes.
	
	\subsection{OpenStreetMap}
	\label{subsec:osm}
	
		\term{OpenStreetMap} (\term*{OSM}) is a geospatial database that is maintained by a global community and licensed under the \term{Open Data Commons Open Database License} (\term*{ODbL})\cite{osm-wiki-about}.
		It can therefore be used, changed and redistributed as long as a proper attribution is given and results stay under the ODbL\cite{odbl-summary}.
		In 2006, two years after the OSM project started, the OpenStreetMap Foundation was established to perform fund-raising, maintain the servers and also act as a legal entity for the project.
		People contributing to OSM are called \textit{mappers} or simply \textit{contributors} and most of them are volunteers, often mapping their local vicinity or concentrating on specific topics.
		However, a rising number of companies with payed mappers contribute as well adding data for their business model \cite{osm-corporate-mappers}.
		
		\subsubsection{Data model}
		
			The model of OSM is much simpler compared to many of the \hyperref[subsec:file-formats]{above mentioned} data formats.
			There are three main data types in OSM: \term{node}, \term{way} and \term[relation]{relations}\cite{osm-wiki-data-model}.
			
			Nodes and ways are analogous to \texttt{Point} and \texttt{LineString} from the OGC \term*{Simple Feature Access} (\term*{SFA}) specification described in \cref{subsec:data-structures}.
			Areas also exist but they are modeled as closed ways, therefore they do not form a new type of geometry.
			A way is closed, when the first and last coordinates are identical and certain area compatible attributes exist.
			
			Relations form multi-geometries, so they are analogous to \texttt{MultiPoint}, \texttt{MultiLineString} and \texttt{MultiPolygon} of the SFA specification.
			Typical use cases are multipolygons, turn restrictions and bus routes.
			Each element in a relation can have the optional attribute \texttt{role} specifying \enquote{the function of a member in the relation}\cite{osm-wiki-relation}.
			Example values for this role are \texttt{outer} and \texttt{inner} for rings in a multipolygon, \texttt{from}, \texttt{via} and \texttt{to} for turn restrictions or \texttt{stop} and \texttt{platform} in a bus route.
			
		\subsubsection{Attributes}
		\label{subsubsec:osm-attributes}
			
			Attributes are called \term[tag]{tags} and are simple unrestricted key-value pairs.
			The Wiki of OpenStreetMap standardizes many tags with designated values via a special proposal process, but it is still allowed to use new, reasonable and unstandardized values \cite{osm-wiki-proposal-process}.
			
			Most tags describe the properties of the feature.
			For example a node can have tags making it a restaurant with additional tags for the name, address, the food to get there, the opening hours and if the toilets are barrier free or not.
			
			Some tags, however, like \texttt{area=[yes|no]} or the \texttt{type} key for relations, influence the type of geometry.
			A closed way with \texttt{area=no} does in fact \textit{not} represent a polygon, but just a closed line.
			This of course affects visualizations but may also need to be considered in other processing and analysis tasks.
			
			Other tags add metadata to objects, for example the last time the feature was surveyed, the source of the data or internal notes to other mappers.
			
			There are two ways a combination of key and value becomes an \enquote{official} tag:
			By using it so often that it is de-facto accepted or by a proposal and vote.
			The proposal process tries to organize and professionalize the development of new tags.
			Not only is a proposed tagging scheme accepted or rejected by a democratic vote, a mandatory public discussion of a minimum duration needs to take place earlier.
			Due to the vast amount of contributors, different opinions, different editors and different knowledge about the tagging schemes, multiple competing schemes might evolve (for example \texttt{phone=*} and \texttt{contact:phone=*}).
			Tagging schemes change over time and some may become deprecated or are officially abandoned via a proposal, leading to outdated tags on objects.
			
		\subsubsection{Contributions to OSM}
		
			Uploads to OSM always happen in so called \term[changeset]{changesets} combining multiple changes on the map\cite{osm-wiki-changeset}, they can therefore be seen as a kind of transaction.
			Each changeset itself can have tags, just like features can.
			However, tags on a changeset specify certain metadata to the change.
			The \texttt{created\_by}, \texttt{comment} and \texttt{source} tags are the most common ones but more details can be added depending on the editor being used.
			
			Because some mappers also perform manual quality assurance by reviews, each changeset should fulfill certain quality criteria.
			The simplest one is a good changeset comment answering questions like \textit{what changed?} and \textit{why was the change necessary?}.
			For legal reasons, specifying the source is very important since the ODbL is not necessarily compatible with licenses of other data sources.
			Ideally a changeset should be coherent, which means that it should focus on one thematic aspect in one local area.
			Too large and crowded changesets are difficult to maintain and therefore might get reverted.
			
		\subsubsection{Data contained in OSM}
		
			Since OSM has no focus on specific topics and due to the flexible tagging scheme \hyperref[subsubsec:osm-attributes]{mentioned above}, nearly anything can be added to OSM.
			However, there are some types of features that are very common according to \href{https://taginfo.openstreetmap.org/keys}{taginfo.openstreetmap.org}, a service providing daily updated statistics on currently used keys and values covering the entire OSM database\cite{taginfo-keys}.
			
			According to these statistics, the most common objects are buildings with over 542 million occurrences.
			In fact 6\% of all objects and nearly 60\% of all ways, which includes polygons as well, in OSM are buildings.
			Highways (mainly roads and streets but also paths, bridleways, railways and more) are the second common type of features with over 233 million occurrences.
			Addresses are also very common, about 32\% of all nodes and 7\% of all ways have a house number.
			Other often added area features are forests and lakes as well as line features like barriers and waterways.
			
		\subsubsection{Data \textit{not} contained in OSM}
		
			Even though the tagging scheme can be extended arbitrarily, some data will probably not be added to OSM.
			This can have multiple reasons.
			
			First, some data is too detailed and therefore people will not invest the time necessary to create or maintain that data.
			For example a tree can have tags specifying its genus, species, taxon, wikipedia and wikidata references to each of these, leaf type, leaf cycle, protection, circumference, height, an image and even more. Some, especially older, protected or locally important trees, do have some of these tags, but the vast majority has none of these tags apart from the necessary \texttt{natural=tree}.
			
			Second, OSM follows some strategies on what to add and what not.
			One important strategy is the verifiability of objects on the ground.
			Anyone visiting a certain place should be able to find the data from OSM and therefore verify its existence and correctness.
			
			Third, temporary data should not be added since OSM is not a real-time database.
			However, there's not a strict definition when a feature is temporary and when it is (potentially) permanent.
			
			And fourth, data under an incompatible license will not be added.
			This also includes data from many public authorities and nearly all companies.
			
			Unfortunately, data relevant to routing is often added to ways representing roads and paths, but not to areas.
			This includes primarily accessibility and surface information.
			However, latter can potentially be inferred or approximated from other tags on a polygon, for example \texttt{surface=grass} can be interred from \texttt{natural=grassland} and \texttt{access=yes} from \texttt{place=square}.
			However, automatic inference of tags always leads to a certain amount of errors.
			
			Private areas often not part of OSM or contain only very little details.
			This is mainly due to reason two from above, since private areas are often unverifiable for unauthorized people, and rooted in a common sense of privacy towards companies and residents.
			
		\subsubsection{Data used in this thesis}
		
			This thesis uses OSM data for routing purposes.
			Since geometric routing is the major topic, not only the road network is used, but also certain features in OSM, which represent obstacles to navigate between.
			The following features are considered as obstalces:
			\begin{itemize}
				\item \textbf{Barriers}: The \texttt{barrier}-tags not only describe non-passable barriers but also passable structures like gates. Thus, only specific barriers (mainly fences and walls) are considered.
				\item \textbf{Buildings}: The \texttt{building} tag as it represents buildings. However, some values, such as \texttt{building=roof}, \texttt{=no} and \texttt{=demolished}, are considered passable.
				\item \textbf{Natural areas}: Areas with the \texttt{natural}-key describe areas filled with a specific form of vegetation as well as water areas. All natural areas, which are not representing grass covered land, are considered impassable.
				\item \textbf{Railway}: Any type of railway infrastructure, especially tracks.
				\item \textbf{Waterways}: This includes any type of waterway, such as rivers, streams, canals, ditches or drains.
			\end{itemize}
			The exact choice on the passable and impassable features is debatable and depends on the domain of the simulation.
			For example, train tracks might be passable in an evacuation scenario.
			
			The road network is also used, which is done by importing all features with a \texttt{highway} key.
			Filtering out irrelevant features is not done as these features have either no effect, which is for example the case for \texttt{highway=street\_lamp} for street lamps on unconnected point geometries, or contain useful information for routing, for example \texttt{highway=stop} for stop signs.

\section{Graph-based routing}
\label{sec:graph-routing}

	\subsection{Theoretic considerations}
	\label{subsec:routing-theoretic-considerations}	
	
		Formally, routing describes the process of finding a path of minimal wight within a graph $G=(V, E)$ from a source $s$ to a vertex $t$ with $s, t \in V$ using a weight function $w: V \rightarrow \mathbb{R}$.
		A path is an ordered list of connected vertices $p=\left\langle v_0, v_1, \dots v_n \right\rangle$, so for two consecutive vertices $v_i$ and $v_j$, $(v_i, v_j) \in E$ must hold.
		
		Often, \enquote{optimal} means shortest, whereby the length of an edge is a numeric value which is not necessarily bound to geographic lengths of edges.
		Instead a weight function $w : E \rightarrow \mathbb{R}$ is used to attach a weight $w(u, v)$ to each edge $(u, v) \in E$.
		Thus routing a minimization problem trying to find a path $p$ with the minimum sum $w(p) = \sum{w(v_{i-1}, v_i)}$ \cite[645]{cormen-introduction-to-alg}.
		
		In theoretic computer science, the fundamental problem behind routing algorithms is called the \term{shortest-path problem}, which exists in several variants\cite[644]{cormen-introduction-to-alg}:
		
		\subsubsection{\term*[single-source shortest path]{Single-source shortest paths}}
		\label{subsubsec:single-source-shortest-path}
		
			One source vertex is given and the shortest paths to all other vertices should be determined.
			Algorithms that solve this problem are for example Dijkstra, which will be described in detail in \cref{subsubsec:dijkstra}, or the Bellman-Ford algorithm, which additionally can handle negative edge weights \cite[651]{cormen-introduction-to-alg}.
		
		\subsubsection{\term*[single-destination shortest path]{Single-destination shortest paths}}
		
			This is the opposite to the above problem.
			All shortest paths to a specific vertex from any other vertex should be found.
			However, no new algorithms are needed to solve this problem.
			Instead, the direction of each edge can be reversed turning this problem into the single-source problem.
		
		\subsubsection{\term*[single-pair shortest path]{Single-pair shortest paths}}
		
			Like the single-source but shortest paths from one source to one destination vertex are requested.
			Even though algorithms like TRANSIT (described in \cref{subsubsec:transit}) solve this problem efficiently, approaches solving the single-source problem, like Dijkstra or algorithms based on it, are used more often.
			% TODO source on that
		
		\subsubsection{\term*[all-pair shortest path]{All-pair shortest paths}}
		\label{subsubsec:all-pair-shortest-path}
		
			This problem is similar to the single-pair problem but is not restricted to a single source vertex.
			A naive approach would be to use an algorithm solving the single-pair problem on each vertex, but there are faster ways to solve this.
			Two popular algorithms solving this problem are the Floyd-Warshall algorithm, allowing negative edge weights by assuming no negative cycles exist \cite[693]{cormen-introduction-to-alg}, and Johnson's algorithm, which also detects negative cycles and terminates in such case \cite[700]{cormen-introduction-to-alg}.
		
	\subsection{Routing engines and shortest path algorithms}
	\label{subsec:routing-engines}
		
		A \term{routing engine} is software built to create an optimal route between a source and destination location.
		The optimality of this route is determined by a routing profile, which is a weighting function assigning a weight to each edge in the input graph.
		Routing engines might perform other operations as well, such as map matching or calculation of isochrones.
		Map matching is a technique to determine equal features in different datasets, for example one dataset is a recorded bicycle tour, the other dataset is an OSM dump and all OSM-ways used in the bicycle tour should be determined \cite{saki-map-matching}.
		Isochrones are a way to visualize the reachability of a region \cite{allen-isochrones}.
		Each isochrone is an area reachable from a given source location within the same amount of time, thus isochrones often form ring like structures.
		Depending on the underlying algorithm, finding optimal paths might be a central algorithmic task to perform map matching or to create isochrones.
		
		To find desirable paths, the weight function $w$ mentioned in \cref{subsec:routing-theoretic-considerations} needs to be carefully chosen based on the domain of the application, vehicle type, personal preferences and other influencing factors.
		Applications in our everyday life bundle these aspects into a \term{routing profile}, which maps attributed from the input data to a certain weight.
		The input data can consist of multiple data sources like a road network graph and a digital elevation model (DEM).
		Common types of weight functions predict the expected speed on an edge \cite{graphhopper-profile-bike-speeds}, use the length of an edge \cite{graphhopper-profile-shortest} or numerically represent a combination of attributes \cite{graphhopper-profile-short-fastest}.
		
		Routing engines, such as Graphhopper, come with predefined profiles for different modalities and situations.
		For example the \texttt{car\_delivery} profile is made for delivery services using cars and therefore allows driving on private roads \cite{graphhopper-routing-profiles}.
		
		To enhance performance, speed up methods were developed to be able to perform fast routing queries on datasets of continental sizes.
		Popular methods are \term{contraction hierarchies} or the use of \term[landmark]{landmarks}, \term[transit node]{transit nodes} and \term[hub label]{hub labels}.
		Even though there are several more approaches to increase performance, the mentioned techniques will be described in \cref{subsec:speedup-methods}.
		
		\subsubsection{Dijkstra}
		\label{subsubsec:dijkstra}
		
			Dijkstra's algorithm, often just called \term{Dijkstra}, is an rather old algorithm from the 1950s solving the single-source shortest paths problem.
			It actually creates a spanning tree rooted in the source vertex $s$ covering all other vertices such that all paths in the tree are shortest paths.
			Despite its age, Dijkstra's algorithm and optimized versions of it are frequently used in science and real world applications and will be mentioned quite often in this thesis.
			\Cref{alg:dijkstra} is an already enhanced version using a priority queue for the vertices \cite[658]{cormen-introduction-to-alg}.
			
			\begin{algorithm}[h]
				\begin{algorithmic}[1]
					\ForAll{vertices $v \in V$}
						\State Set shortest path distance $v.d = \infty$ and predecessor vertex $v.\pi = undefined$
					\EndFor
					\State Insert all vertices by distance into min-queue $Q$
					\State
					\While{$Q \neq \emptyset$}
						\State Get closest vertex: $u = Q.min$
						\ForAll{adjacent vertices $v$ of $u$}
							\If{the path to $v$ via $u$ is shorter than $v.d$, so if $u.d + d(u, v) < v.d$}
								\State Set $v.d = u.d + d(u, v)$ and $v.\pi = u$
							\EndIf
						\EndFor \label{alg:dijkstra:end-inner-loop}
					\EndWhile
				\end{algorithmic}
				\caption{Pseudocode of an slightly optimized version of Dijkstra's algorithm.}
				\label{alg:dijkstra}
			\end{algorithm}
		
			Once a vertex is taken from the queue, it is considered as \emph{visited}.
			It can be proven that for each visited vertex $v$ after the inner for-loop, the distance $v.d$ is optimal \cite[659-661]{cormen-introduction-to-alg}.
			This also means following back the predecessor relation $v.\pi$ yields the shortest path.
			Therefore, stopping the algorithm after line \ref{alg:dijkstra:end-inner-loop} when $u$ is equal to the target vertex $t$ is possible since the optimal path to $t$ has been found.
		
		\subsubsection{A*}
		\label{subsubsec:astar}
		
			The A* shortest path algorithm was introduced in 1968 and uses a heuristic $h : V \rightarrow \mathbb{R}$ in combination with a distance $g : V \rightarrow \mathbb{R}$ to decide what vertex to visit next \cite{astar}.
			The heuristic $h$ estimates the distance from a node to the target node $t$, while the distance function $g$ gives the already determined distance from the start node $s$ to a given node.
			Summing up the two functions to $f(n) = g(n) + h(n)$ for a given node $n$, gives the estimated length of a shortest path from $s$ via $n$ to $t$ and is used to decide what successor of $n$ to process next.
			The structure of A* is rather simple and \cref{alg:astar} expresses it in pseudocode.
			
			\begin{algorithm}[h]
				\begin{algorithmic}[1]
					\State Mark $s$ as \emph{open}
					\State Set $u = s$ since $s$ is the only open vertex
					\While{$u \neq t$}
						\State Mark $u$ as \emph{closed}
						\State Store $u$ with an edge to its predecessor to output graph $G$
						\State Mark all non-closed successors of $u$ as \emph{open} \label{alg:astar-open-1}
						\State Mark each closed successor $u'$ of $u$ as \emph{open} when $f(u')$ became lower since the last visit where it has been marked as \emph{closed} \label{alg:astar-open-2}
						\State Select open vertex $u$ with minimal $f(u)$
					\EndWhile
					\State Take $G$, follow $t$ back to the source $s$ and output the path
				\end{algorithmic}
				\caption{Pseudocode of the originally proposed A* algorithm \cite{astar}.}
				\label{alg:astar}
			\end{algorithm}
			
			The performance of A* relies heavily on the quality of the heuristic and therefore no general complexity formula can be given \cite{russell-norvig-ai-modern-approach}.
			However, given a specific problem, a so called \term{effective branching factor} $b^*$ can be determined which approximates the number of new open nodes per processed node (see line \ref{alg:astar-open-1} and \ref{alg:astar-open-2} in \cref{alg:astar}).
			The optimal branching factor is 1 and a good heuristic has a factor of close to 1.
			Having $b^*$ and the number of nodes $d$ in the shortest path, a general runtime complexity of $\bigo{(b^*)^d}$ can be assumed.
			Since all closed nodes have to be stored in the output graph $G$, the space complexity is equal to the time complexity.
			This is the major drawback of A* since a bad heuristic can turn A* into an algorithm with exponential and therefore unmanageable complexity.
		
	\subsection{Speedup methods}
	\label{subsec:speedup-methods}
		
		\subsubsection{Contraction Hierarchies}
		\label{subsubsec:ch}
		
			Contraction hierarchies are based on the idea to reduce the amount vertices a routing algorithm has to visit by adding so called \emph{shortcut edges} \cite{geisberger-contraction-hierarchies}.
			Consider a vertex $u$ with incoming edge $(v, u)$ and outgoing edge $(u, w)$, meaning there is a path from $v$ via $u$ to $w$.
			Vertex $u$ is \emph{contracted} by adding a new edge $e = (v, w)$ with weight $w(e) = w(v, u) + w(u, w)$.
			If $e$ already exists with a higher weight, its weight is decreased accordingly.
			
			A very important part of this technique is the selection of vertices to contract \cite[14]{geisberger-contraction-hierarchies}.
			The vertices are selected by a total order $<$ defining a level of importance for each vertex.
			Selecting the correct order is difficult and heuristics are used to approximate a perfect ordering.
			Such heuristic can for example be the difference in edge counts from before and after contraction, the uniformity of the distribution of contractions in the Graph or the resulting size of the query search space.
			Multiple heuristics can be combined leading to even better results \cite[49]{geisberger-contraction-hierarchies}.
			The vertices are stored in a priority queue based on a linear combination of all used heuristics and the most attractive vertex is processed first.
			
			The contraction hierarchy can be used with an adjusted bidirectional Dijkstra algorithm \cite[29-30]{geisberger-contraction-hierarchies}.
			A query for the shortest path is done on two graphs, an upward and downward graph.
			The upward graph contains only edges from lower to higher order vertices and the downward graph only from higher to lower order vertices.
			The query finishes, when the two queries (bidirectional) meet \emph{and} there is no lower weighted edge to vertices still waiting in the priority queue.
			All shortcut edges must now be recursively relaxed to create the actual shortest path.
		
		\subsubsection{Landmarks}
		
			A popular usage of so called \term[landmark]{landmarks} is the ALT algorithm, which stands for A*, landmarks and triangle inequality \cite{goldberg-landmarks}.
			As described in \cref{subsubsec:astar}, the A* algorithm needs a heuristic leading the search for the shortest path towards the target vertex.
			This heuristic is often the euclidean distance divided by a certain speed, which is simple but rather inaccurate.
			
			A better, but also more complex, approach is to use a set of landmarks $L$.
			Landmarks are vertices to which all distances are calculated in a precomputation step.
			
			Consider a distance function $d : V \times V \rightarrow \mathbb{R}$, two vertices $u, v \in V$ as well as a landmark $l \in L$.
			The triangle inequality says $d(u,v) \geq d(u,l) - d(v,l)$ for the distances \emph{to} $l$ but also $d(u,v) \geq d(l,u) - d(l,v)$ for distances \emph{from} $l$.
			This can be used to build the heuristic $h(u) = \max \{ d(u,l)-d(v,l), d(l,u)-d(l,v) \}$ for all landmarks $l$.
			
			The critical part regarding the landmarks is their selection.
			A naive approach selects them at random, which already works, but selecting them in a way that maximized the minimum distance between the landmarks works better.
			
			Thanks to the triangle inequality, the A* algorithm can use this to determine good choices for next vertices.
			Since the heuristic consist of the tightest bound by using the $\max$ function, the estimated distance to $t$ using the heuristic is likely to increase or decrease when the actual distance does the same.
		
		\subsubsection{Transit nodes}
		\label{subsubsec:transit}
		
			The routing algorithm called \term{TRANSIT} uses specially selected vertices, so called \term[transit node]{transit nodes}, and the distances to all other vertices in the graph to find shortest paths very quickly \cite{bast-transit}.
		
			First, the graph is divided into smaller rectangular cells and Dijkstra queries are then used to determine the transit nodes.
			For each cell, the neighboring cells of a $5 \times 5$ (inner neighbors) and $9 \times 9$ (outer neighbors) neighborhood are determined.
			Determining transit nodes in a naive way works as follows:
			For every vertex within the center cell, shortest paths to all vertices in the outer cells are determined.
			A vertex being on an shortest path edge leading from an inner into an outer cell is considered a transit node.
			Since Dijkstra solves the \hyperref[subsubsec:single-source-shortest-path]{single-source shortest paths problem}, all distances from a normal vertex to its nearest transit nodes are stored.
			The final step of the preprocessing is the calculation of the distances between all transit nodes, which means solving the \hyperref[subsubsec:all-pair-shortest-path]{all-pair shortest path problem}.
			
			For a source vertex $src \in V$ and destination vertex $dst \in V$, the distances from $src$ to its transit nodes $T_{src} \subset V$, the distances from $dst$ to it transit nodes $T_{dst} \subset V$ and also the distances between the vertices from $T_{src}$ and $T_{dst}$ are known.
			To determine the shortest path, these distances are used including the total shortest path distance $d(src, dst)$.
			
			The first routine calculates this distance $d(src, dst)$.
			For each closest transit node $t_{src} \in T_{src}$ of $src$, all distances $d(src, t_{src})$ were already precomputed an known.
			Same holds for all distances $d(t_{dst}, dst)$ from each closest transit node $t_{dst} \in T_{dst}$ to $dst$.
			The minimal distance between $src$ and $dst$ can be found by going through all pairs of the closest transit nodes $t_{src} \in T_{src}$ and $t_{dst} \in T_{dst}$ and calculating $d(src, dst) = d(src, t_{src}) + d(t_{src}, t_{dst}) + d(t_{dst}, dst)$.
			
			With the total distance $d(src, dst)$ of the shortest path, it is possible to iteratively pick the next vertex in the shortest path.
			For each vertex $v$ on the shortest path, the next vertex $v'$ on that path can be found as follows:
			the one neighbor vertex $v'$ for which $d(src, v) + d(v, v') + d(v', dst) = d(src, dst)$ holds is the next vertex on the shortest path.
			Therefore, it is essential to know the distance $d(src, dst)$.
			After reaching $dst$, the list of stored vertices build the shortest path.
		
		\subsubsection{Hub labels}
		
			The \term{hub label} strategy \cite{bast-transportation-networks} is a labeling algorithm calculating labels $L(v)$ for each vertex $v$.
			Each label contains a set of vertices (so called \emph{hubs}) and the length of the shortest path to each of the hubs.
			A label must fulfill the following two properties.
			
			First, the distance $d(u, v)$ from vertex $u$ to $v$ must be determinable using only their labels $L(u)$ and $L(v)$.
			Since $u$ and $v$ can be arbitrary vertices, the vertex $v$ is not necessarily within $L(u)$.
			To not traverse through all labels to find $v$ from $u$, the second property (so called \emph{cover property}) is very important:
			For two arbitrary vertices $u$ to $v$, the set $L(u) \cap L(v)$ must contain at least one common hub $h$ on the shortest path from $u$ to $v$.
			This can be used to solve fulfill the first property.
			
			The labels can be determined with the mechanism from contraction hierarchies (see \hyperref[subsubsec:ch]{above}) forming the upward graph.
			In fact, any order on vertices can be used to create a correct labeling.

			Routing requests for a shortest path from $u$ to $v$ are simple and fast to answer:
			Get the labels $L(u)$ and $L(v)$ and find all common hubs.
			When storing the hubs in a sorted list during preprocessing, building the intersection can be done in linear time time.
			Find the hub pair $h_u$ and $h_v$ in the intersection with the minimum distance sum and return their paths.
			
			Even though the requests can be answered incredibly fast, actually faster than most other routing methods, it needs a lot of space and preprocessing time to store and determine all the labels.
		
		\subsubsection{Compressed path databases}
		\label{subsubsec:cpd}
		
			A \term{compressed path database} (\term*{CPD}) itself is not directly a routing algorithm but rather a technique to efficiently store shortest paths between any two locations of a domain \cite{botea-cpd-2013}.
			In other words, the solution of the all-pair shortest paths problem from \cref{subsubsec:all-pair-shortest-path} can be stored in a CPD without having extensive memory requirements.
			
			The simplest form of a CPD works on a grid rather than a graph, where the number of neighbors is limited and visualization, calculations and compression is simpler.
			Such grid-based CPDs actually consist not of one but of $n$ many grids, each one storing movement information from a fixed cell $v$ to all other cells.
			When having $n$ cells per grid, $n$ grids are needed so that for each cell, one grid with movement information exists.
			However, movement information here means, that only a single step is stored in a grid.
			
			Consider the following example:
			The path from cell $u$ to $v$ is requested, so the grid $u$ is used first.
			On that grid, the cell for $u$ is empty (since it is the source cell) and the cell for $v$ stores the first step to take from $u$ in order to get to $v$.
			This first step (on a grid for example the direction \enquote{north east}) leads to a new cell $u'$.
			The grid for $u'$ is then looked at and, just as before, the cell $v$ tells the algorithm what direction to take.
			This goes on until the cell $v$ is found.
			
			The construction of a CPD \cite{botea-cpd-2013} is similarly simple.
			For each vertex $v$, the shortest paths to all other vertices are calculated, which can be done using Dijkstra or any other single-source or all-pairs shortest path algorithm.
			After converting the shortest paths into a compatible format (in the above case a grid), the data needs to be compressed.
			Uncompressed data is usually too large as the space requirement is in $\bigo{n^2}$ or even $\bigo{n^2 \log n}$ for graphs.
			
			There are multiple possible ways to compress a raw CPD.
			The original paper uses a decomposition of rectangles with common movement directions to reduce the amount of redundant data.
			A new approach presented two years later uses general purpose compression techniques like run-length encoding (RLE) and sliding-window compression (SWC) as well as a special list-trimming using default movements to reduce the amount of information being explicitly stored.
			
			The original approach already showed good results.
			Using the originally proposed compression (rectangle decomposition only) on a road network yields a CPD with a compression factor of about 180.
			Compressing the same network with all mentioned techniques together can decrease the size even further to a CPD, which is another 5.3 times smaller.
			This is a compression factor of over 950 compared to an uncompressed database.
	
\section{Geometric routing}
\label{sec:geometric-routing}

	Geometric routing determines shortest paths through open spaces by avoiding obstacles.
	Because this way of routing is based on geometric properties, attributes of edges (for example the surface conditions of roads) are of no use.
	There are to main strategies for finding shortest paths:
	Create edges through open spaces for graph-based routing or create a shortest paths map, a structure similar to CPDs.
	
	\subsection{Visibility graphs}
	\label{subsec:visibility-graph}
	
		The first mechanism creates edges and then utilizes a normal graph-based routing algorithm to actually find the shortest path.
		There are multiple ways on how to create such edges.
		A well known and often used mechanism is the creation of a so called \term{visibility graph}, which is a normal graph with edges between vertices that are visible to each other.
		Or in other words, for any two $u, v \in V$ there is an edge $(u, v) \in E$ if and only if there is no obstacle intersecting this edge.
		Since a visibility graph might become very large, some edges might be removed or other strategies are more efficient.
		In fact, the worst-case of a visibility graph is a complete graph with $\bigo{|V|^2}$ many edges.
		
		Alternatives to the visibility method are based on Voronoi diagrams or skeletonization methods.
		However, routing results on such graphs are not be optimal anymore, since a straight edge between visible vertices is the shortest possible connection and those alternatives so not always create such direct edges \cite{graser-osm-open-spaces}.
		Another property of graph generating methods is, that the start and destination locations must still be on the graph to be precisely reachable by routing algorithms.
	
	\subsection{Continuous Dijkstra}
	\label{subsec:continuous-dijkstra}
	
		The second strategy creates a map of regions starting from a source vertex $s$.
		Each region of this map stores the predecessor vertex, which can be used to determine shortest paths by following the predecessor relations from any given location back to $s$.

		This map can also be seen as a tree with $s$ as its root and each region as a leaf.
		Such a tree might remind one of Dijkstras algorithm, which creates such a shortest path tree as well.
		Due to this similarity, this strategy is also called the \term{continuous dijkstra} paradigm \cite{mitchell-discrete-geodesic}.
		
		To precisely determine these regions, one popular approach is to create \term[wavefront]{wavefronts} (sometimes also called \term[wavelet]{wavelets}) propagating through open spaces.
		A well fitting analogy for this approach is the propagation of sound or water waves traveling through space, folding around obstacles and finally reaching the destination location.
		Recursively following each wave back to its origin yields the shortest path from the source to the destination.
		
		Each wavefront can interact with edges and vertices but also with other wavelets.
		Interactions in this case are collisions creating the borders of the regions.
		For each region the origin of the wavefront is known, which enables the query routine to follow these predecessors back to the source.
		The key difficulty of this approach is the efficient propagation of these wavefronts, which means the fast detection of collision with obstacles and other wavefronts \cite{hershberger-suri}.

\section{Agent-based systems and simulations}

	Simulating complex systems with numerous individuals is a complex task.
	Agent-based models help to break down this complexity by simulating each individual separately with its own behavior and decision-making \cite{macal-introductory-tutorial}.
	Interactions between agents and the environment as well as agents with other agents are an essential part of agent-based simulations.
	Each interaction can affect other agents and the environment.
	
	The modeling of agents can be arbitrarily complex.
	Basic agents can interact with other agents and the environment.
	They are able to make decisions on their own, meaning they are autonomous since there is no controlling unit commanding them how to act.
	More complex agents may have a specific goal, are able to adapt themselves to their environment and thus must be able to memorize and plan things.
	
	An agent might be a simulated person but could also be a company, car or other non-living part of the simulated world.
	Many simulations use pedestrians (or generally humans) as agents to better understand human behaviors but many more scenarios are thinkable \cite{macal-introductory-tutorial}.

	Studying the behavior of pedestrians requires a spatial environment where agents can move and interact with each other.
	Since uncontrolled movement is rather uninteresting, most of the time, algorithms finding optimal paths are involved \cite{kneidl-borrmann-hartmann-navigation,gloor-hybrid-pedestrian-routing,teknomo-millonig-routing}.
	This not only includes \hyperref[sec:graph-routing]{graph-based routing} but often utilizes \hyperref[sec:geometric-routing]{geometric routing} as well \cite{kneidl-borrmann-hartmann-navigation}.
	
	% TODO Describe MARS here or later?